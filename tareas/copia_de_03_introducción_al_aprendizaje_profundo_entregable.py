# -*- coding: utf-8 -*-
"""Copia de 03_Introducci√≥n_al_aprendizaje_profundo_entregable.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D1sVIS4NK5w0dm1E_XDRTXn81lYUQzHh

# **Aprendizaje profundo - Sesi√≥n 3  üß†**

> **Descripci√≥n:** Cuaderno de contenidos del m√≥dulo de aprendizaje profundo para el Dimplomado en Ciencia de Datos de la ENES UNAM Le√≥n, 2024. <br>
> **Autor:** [Rodolfo Ferro](https://github.com/RodolfoFerro) <br>
> **Contacto:** [ferro@cimat.mx](mailto:ferro@cimat.mx)


## Contenido

### Secci√≥n I

1. Overfitting vs. Underfitting
2. Regularizaci√≥n:
    - Dropout
    - Early Stopping

### Secci√≥n II

3. Problemas de regresi√≥n

### Secci√≥n III

4. Problemas de clasificaci√≥n

## **Secci√≥n I**

### **IMPORTANTE**

El contenido de esta secci√≥n ha sido descrito en su totalidad a trav√©s de la presentaci√≥n.

Conviene revisar el material que puedes encontrar en el [repositorio](https://github.com/RodolfoFerro/modulo-deep-learning).

### **Ejemplo de Early Stopping**
"""

import tensorflow as tf
import numpy as np


# Sample data (XOR)
x = np.array([(0, 0), (1, 0), (0, 1), (1, 1)])
y = np.array([0, 1, 1, 0])


# Creates the model
model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(2, activation='linear'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])


# Compile the model
loss = tf.keras.losses.MeanSquaredError()
optimizer = tf.keras.optimizers.SGD(learning_rate=0.6)

model.compile(optimizer=optimizer, loss=loss, metrics=[loss])

# Create a callback
callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)

history = model.fit(x, y, epochs=1000, callbacks=[callback])

import plotly.express as px


losses = history.history['loss']
eje_x = np.arange(len(losses))

fig = px.line(
    x=eje_x,
    y=losses,
    title='Historia de entrenamiento',
    labels=dict(x='√âpocas', y='Error')
)
fig.show()

"""## **Secci√≥n II**

### **Regresi√≥n lineal 01**

Resolveremos este primer ejercicio creando una red neuronal sencilla con datos sint√©ticos.
"""

import numpy as np

x = np.linspace(0, 100, 101)
x

y = x + 10 * np.random.random((len(x)))
y

import plotly.express as px


fig = px.scatter(x=x, y=y)
fig.show()

"""### **Modelo**

Por la simplicidad de los datos, podemos intentar realizar el ajuste de una recta, por lo que una simple neurona artificial bastar√° para modelar la ecuaci√≥n $y=mx+b$.

El peso entrenado corresponder√° a la pendiente $m$ de la ecuaci√≥n y el _bias_ al valor de intersecci√≥n $b$.
"""

import tensorflow as tf

model = tf.keras.Sequential()
model.add(tf.keras.layers.Input([1])) # Hint: Input layer is Input([1])
model.add(tf.keras.layers.Dense(1,activation='linear')) # Hint: Output layer is Dense(1) w/linear activation

model

"""### **Optimizador y funci√≥n de p√©rdida**

$$ \mathrm{MSE}=\frac{1}{N}\cdot\sum_{i=1}^N \left(y_i- \hat{y}_i \right )^2 $$
"""

# TODO: Create loss function and optimizer
loss = tf.keras.losses.MeanSquaredError()
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-1) # Try SGD(learning_rate=1e-5) / Adam(learning_rate=1e-1)

"""> **¬øQu√© es ADAM?** $\rightarrow$ https://www.geeksforgeeks.org/adam-optimizer/"""

# TODO: Add optimizer and loss to model compilation
model.compile(loss=loss, optimizer=optimizer, metrics=[loss])

model.summary()

"""### **Entrenamiento del modelo**"""

history = model.fit(x, y, epochs=100)

import plotly.express as px


losses = history.history['loss']
eje_x = np.arange(len(losses))

fig = px.line(
    x=eje_x,
    y=losses,
    title='Historia de entrenamiento',
    labels=dict(x='√âpocas', y='Error')
)
fig.show()

# Build output over original x
y_pred_model = model.predict(x)

import plotly.graph_objects as go


fig = go.Figure()
fig.add_trace(
    go.Scatter(x=x, y=y, mode='markers', name='Datos crudos')
)
fig.add_trace(
    go.Scatter(x=x, y=y_pred_model.flatten(), mode='lines', name='Modelo ajustado')
)

fig.show()

"""### **Evaluaci√≥n del modelo**"""

from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score


# MSE
y_pred_model = model.predict(x)
print("MSE:", mean_squared_error(y, y_pred_model))

# R2
print("R^2:", r2_score(y, y_pred_model))

"""> - **M√©tricas de regresi√≥n:** https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics
> - **Coeficiente de determinaci√≥n:** https://en.wikipedia.org/wiki/Coefficient_of_determination
"""

w, b = model.get_weights()
m, b = w[0][0], b[0]
m, b

"""### **Reconstrucci√≥n del modelo**"""

y_reconstructed = x * m + b

import plotly.graph_objects as go


fig = go.Figure()
fig.add_trace(
    go.Scatter(x=x, y=y, mode='markers', name='Datos crudos')
)
fig.add_trace(
    go.Scatter(x=x, y=y_reconstructed, mode='lines', name='Modelo ajustado')
)

fig.show()

"""---

### **Regresi√≥n 02**

Ahora vamos a resolver un problema de regresi√≥n no necesariamente lineal.
"""

import numpy as np


x = np.linspace(-4 * np.pi, 4 * np.pi, 10000)
y = np.sin(x) + 0.5 * np.random.randn(len(x))

import plotly.express as px


fig = px.scatter(x=x, y=y)
fig.show()

"""### **Modelo**"""

import tensorflow as tf


# TODO: Build the model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Input([1])) # Hint: Input layer is Input([1])
# TODO: Create at least 1 hidden layer

model.add(tf.keras.layers.Dense(64,activation='relu'))
model.add(tf.keras.layers.Dense(128,activation='relu'))
model.add(tf.keras.layers.Dense(64,activation='relu'))
model.add(tf.keras.layers.Dense(32,activation='relu'))



model.add(tf.keras.layers.Dense(1,activation='linear')) # Hint: Output layer is Dense(1) w/linear activation

"""### **Optimizador y funci√≥n de p√©rdida**

$$ \mathrm{MSE}=\frac{1}{N}\cdot\sum_{i=1}^N \left(y_i- \hat{y}_i \right )^2 $$
"""

# TODO: Create loss function and optimizer
loss = tf.keras.losses.MeanSquaredError()
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3) # Try Adam(learning_rate=1e-3)

# TODO: Integrate all here!
model.compile(loss=loss, optimizer=optimizer, metrics=[loss])

model.summary()

"""### **Entrenamiento del modelo**"""

history = model.fit(x, y, epochs=50)

import plotly.express as px


losses = history.history['loss']
eje_x = np.arange(len(losses))

fig = px.line(
    x=eje_x,
    y=losses,
    title='Historia de entrenamiento',
    labels=dict(x='√âpocas', y='Error')
)
fig.show()

"""### **Reconstrucci√≥n del modelo**"""

# Build output over original x
y_pred_model = model.predict(x)

import plotly.graph_objects as go


fig = go.Figure()
fig.add_trace(
    go.Scatter(x=x, y=y, mode='markers', name='Datos crudos')
)
fig.add_trace(
    go.Scatter(x=x, y=y_pred_model.flatten(), mode='lines', name='Modelo ajustado')
)

fig.show()

"""<center>
    *********
</center>

## **Secci√≥n III ‚Äì Ejercicio**

### **Clasificaci√≥n 01**

Para este problema utilizaremos un dataset sint√©tico ya conocido:
"""

from sklearn.datasets import make_circles
import numpy as np


# Make 1000 examples
n_samples = 1000

# Create circles
x, y = make_circles(n_samples, noise=0.03, random_state=42)

import pandas as pd


circles = pd.DataFrame({"x1": x[:, 0], "x2": x[:, 1], "label":y})
circles.head()

# Verify number of labels
circles.label.value_counts()

import plotly.express as px


fig = px.scatter(
    x=circles['x1'],
    y=circles['x2'],
    color=circles['label'].astype(str) # Discretize the color palette
)
fig.show()

"""### **Modelo**"""

import tensorflow as tf


# TODO: Create the model using the Sequential API
# Hint - You can try:
#  - Input(2)
#  - Hidden layers - tanh
#  - Output layer(1) - sigmoid
model = tf.keras.Sequential()
model.add(tf.keras.layers.Input([2]))
model.add(tf.keras.layers.Dense(16,activation='relu'))
model.add(tf.keras.layers.Dense(32,activation='tanh'))
model.add(tf.keras.layers.Dense(1,activation='sigmoid'))

"""### **Optimizador y funci√≥n de p√©rdida**

$$ \mathrm{Binary Crossentropy}=-\frac{1}{N}\cdot\sum_{i=1}^N \left[y_i \log (\hat{y}_i) + (1 - y_i) \log (1 - \hat{y}_i) \right] $$
"""

# TODO: Create loss function and optimizer
loss = tf.keras.losses.BinaryCrossentropy()
optimizer = tf.keras.optimizers.Adam(learning_rate=3e-3)
# Try Adam(learning_rate=3e-3)

# TODO: Integrate all here!
model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])

model.summary()

"""### **Entrenamiento del modelo**"""

history = model.fit(x, y, epochs=30)

import plotly.express as px


losses = history.history['loss']
eje_x = np.arange(len(losses))

fig = px.line(
    x=eje_x,
    y=losses,
    title='Historia de entrenamiento',
    labels=dict(x='√âpocas', y='Error')
)
fig.show()

import plotly.express as px


losses = history.history['accuracy']
eje_x = np.arange(len(losses))

fig = px.line(
    x=eje_x,
    y=losses,
    title='Historia de entrenamiento',
    labels=dict(x='√âpocas', y='Accuracy')
)
fig.show()

"""### **Evaluaci√≥n del modelo**"""

model.evaluate(x, y)

y_pred = model.predict(x)

def binarize_output(y_pred, threshold=0.5):
    return np.array([1 if x > threshold else 0 for x in y_pred])

from sklearn.metrics import accuracy_score


y_pred_bin = binarize_output(y_pred)
print("Accuracy score:", accuracy_score(y, y_pred_bin))

"""> - **M√©tricas de clasificaci√≥n:** https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics"""

import plotly.express as px


fig = px.scatter(
    x=circles['x1'],
    y=circles['x2'],
    color=y_pred_bin.flatten().astype(str) # Discretize the color palette
)
fig.show()

"""---

### **Clasificaci√≥n 02**

Ahora utilizaremos un conjunto de datos un poco m√°s complejo: el **MNIST dataset**.

El dataset est√° compuesto por im√°genes de 28x28 pixeles, que contienen un conjunto de d√≠gitos en 10 categor√≠as.

Los datos de MNIST est√°n disponibles directamente en la API de conjuntos de datos de `tf.keras`. Los cargas as√≠:
"""

import tensorflow as tf

mnist = tf.keras.datasets.mnist

"""Llamar a `load_data` en este objeto nos dar√° dos conjuntos con los valores de entrenamiento y prueba para los gr√°ficos que contienen las prendas y sus etiquetas."""

(training_images, training_labels), (test_images, test_labels) = mnist.load_data()

"""¬øC√≥mo se ven estos valores?

Imprimamos una imagen de entrenamiento y una etiqueta de entrenamiento para ver.
"""

import numpy as np
import matplotlib.pyplot as plt
np.set_printoptions(linewidth=200)


# Set index of image to be seen
img_index = 5999 # 6000 -1

# Plot image
plt.imshow(training_images[img_index], cmap='gray')
plt.axis(False)

print('Label:', training_labels[img_index])
print('Matrix:', training_images[img_index])

"""### **Preparaci√≥n de los datos**

Notar√°s que todos los valores est√°n entre 0 y 255. Si estamos entrenando una red neuronal, por varias razones es m√°s f√°cil si transformamos los valores para tratar todos con valores entre 0 y 1. Este proceso se llama **estandarizaci√≥n**.
"""

training_images  = training_images / 255.0
test_images = test_images / 255.0

import numpy as np
import matplotlib.pyplot as plt
np.set_printoptions(linewidth=200)


# Set index of image to be seen
img_index = 3000 # 6000 -1

# Plot image
plt.imshow(training_images[img_index], cmap='gray')
plt.axis(False)

print('Label:', training_labels[img_index])
print('Matrix:', training_images[img_index])

training_images[0].shape

"""### **Modelo**


"""

# TODO: Create the model using the Sequential API
# Hint - You can try:
#  - Flatten()
#  - Hidden layers - relu
#  - Output layer(10) - softmax
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))
model.add(tf.keras.layers.Dense(128,activation='relu'))
model.add(tf.keras.layers.Dense(128,activation='softmax'))

"""> **Softmax Activation Function: Everything You Need to Know** - https://www.pinecone.io/learn/softmax-activation/

### **Optimizador y funci√≥n de p√©rdida**
"""

# TODO: Create loss function and optimizer
loss = tf.keras.losses.SparseCategoricalCrossentropy()
optimizer = tf.keras.optimizers.SGD()
 # Try SGD()

# TODO: Integrate everything here!
model.compile(
    optimizer=optimizer,
    loss=loss,
    metrics=['accuracy']
)

"""### **Entrenamiento del modelo**

Para entrenar el modelo, simplemente utilizamos el m√©todo `.fit()` del modelo.
"""

history = model.fit(training_images, training_labels, epochs=10)

"""
> **Pregunta clave:** ¬øQu√© sucede con la historia de entrenamiento?"""

import plotly.express as px


seen = 'accuracy' # or 'loss'

hist_values = history.history[seen]
eje_x = np.arange(len(hist_values))

fig = px.line(
    x=eje_x,
    y=hist_values,
    title='Historia de entrenamiento',
    labels=dict(x='√âpocas', y=seen.capitalize())
)
fig.show()

"""### Evaluaci√≥n del modelo"""

model.evaluate(test_images, test_labels)

"""### Predicci√≥n

"""

import random

test_index = random.randint(0, 10000 - 1)

plt.imshow(test_images[test_index], cmap='gray')
plt.axis(False)

print('Label:', test_labels[test_index])
input_image = np.reshape(test_images[test_index], (1, 784))
prediction = model.predict(np.expand_dims(input_image, axis=-1))
print('Prediction:', np.argmax(prediction))

prediction

model.summary()

"""> **Para resolver el reto es:** Mejorar el accuracy obtenido en la clase.

**Puedes explorar:**
- El n√∫mero de capas.
- Las √©pocas de entrenamiento.
- Las funciones de activaci√≥n.
- Investigar otras capas.

--------

> Contenido creado por **Rodolfo Ferro**, 2024. <br>
> Para cualquier retroalimentaci√≥n, puedes contactarme a trav√©s del correo [ferro@cimat.mx](mailto:ferro@cimat.mx).
"""